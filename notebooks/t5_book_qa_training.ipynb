{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# T5 Fine-tuning for Book Question Answering\n",
    "\n",
    "This notebook demonstrates how to fine-tune T5-small model on the Katharinelw/Book dataset for domain-specific question answering, following The Stanford Question Answering Dataset (SQuAD) standards.\n",
    "\n",
    "## Key Features\n",
    "- **Generative Q&A**: Fine-tune T5-small for generating answers to book-related questions\n",
    "- **SQuAD-compatible format**: Data preprocessing to match SQuAD standards\n",
    "- **Optimized for Colab**: GPU-accelerated training with memory optimization\n",
    "- **Comprehensive evaluation**: BLEU, ROUGE metrics for generative QA\n",
    "\n",
    "## Training Steps\n",
    "1. Define task scope (generative Q&A for book domain)\n",
    "2. Prepare domain data (Q&A pairs in SQuAD-like format)\n",
    "3. Configure T5-small model\n",
    "4. Format inputs/targets for text-to-text learning\n",
    "5. Train on subset first (sanity check), then extend\n",
    "6. Evaluate with meaningful metrics\n",
    "7. Test integration capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers datasets accelerate evaluate rouge-score nltk pandas scikit-learn tqdm\n",
    "\n",
    "# Check if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ÑπÔ∏è Running in local environment\")\n",
    "\n",
    "# Clone the repository and navigate to it (only in Colab)\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüì• Cloning repository...\")\n",
    "    !git clone https://github.com/wedsamuel1230/finetune-test.git\n",
    "    %cd finetune-test\n",
    "    print(\"‚úÖ Repository cloned and ready!\")\n",
    "    \n",
    "    # Mount Google Drive for model saving (optional)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted at /content/drive\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è Local environment detected - assuming repository is already available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "**‚ö†Ô∏è Important for Google Colab Users:**\n",
    "\n",
    "1. **Enable GPU**: Go to `Runtime > Change runtime type > Hardware accelerator > GPU (T4)`\n",
    "2. **Restart Runtime**: After installing packages above, go to `Runtime > Restart runtime` then continue from the next cell\n",
    "3. **Google Drive**: Models will be saved to Google Drive for persistence across sessions\n",
    "4. **File Uploads**: You can upload custom datasets using the file upload feature below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_check"
   },
   "outputs": [],
   "source": [
    "# Re-check environment after potential restart\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "    \n",
    "    # Navigate to cloned directory if needed\n",
    "    import os\n",
    "    if not os.path.exists('/content/finetune-test'):\n",
    "        print(\"\\nüì• Repository not found, cloning...\")\n",
    "        !git clone https://github.com/wedsamuel1230/finetune-test.git\n",
    "    \n",
    "    %cd /content/finetune-test\n",
    "    print(\"üìÅ Working directory:\", os.getcwd())\n",
    "    \n",
    "    # Optional: File upload widget for custom datasets\n",
    "    from google.colab import files\n",
    "    print(\"\\nüì§ To upload custom dataset files, run: uploaded = files.upload()\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ÑπÔ∏è Running in local environment\")\n",
    "    import os\n",
    "    print(\"üìÅ Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "from typing import List, Dict\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Check GPU availability and configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Memory: {memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Optimize for Colab T4 GPU (15GB)\n",
    "    if memory_gb < 16:\n",
    "        print(\"\\n‚öôÔ∏è Detected T4 GPU - applying memory optimizations...\")\n",
    "        torch.backends.cudnn.benchmark = False  # Save memory\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # Prevent fragmentation\n",
    "    \n",
    "    # Clear cache to start fresh\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU memory cache cleared\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è GPU not available. Training will be slow on CPU.\")\n",
    "    print(\"   In Colab: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### üìÅ Custom Dataset Upload (Optional)\n",
    "\n",
    "If you want to use your own dataset in Google Colab, uncomment and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "file_upload"
   },
   "outputs": [],
   "source": [
    "# Optional: Upload custom dataset files in Colab\n",
    "# Uncomment the lines below if you want to upload your own book dataset\n",
    "\n",
    "# if IN_COLAB:\n",
    "#     from google.colab import files\n",
    "#     print(\"üì§ Upload your custom dataset files (JSON format):\")\n",
    "#     uploaded = files.upload()\n",
    "#     \n",
    "#     for filename in uploaded.keys():\n",
    "#         print(f\"‚úÖ Uploaded: {filename} ({len(uploaded[filename])} bytes)\")\n",
    "#     \n",
    "#     # Example: Load custom dataset\n",
    "#     # import json\n",
    "#     # with open(list(uploaded.keys())[0], 'r') as f:\n",
    "#     #     custom_data = json.load(f)\n",
    "#     #     print(f\"Loaded {len(custom_data)} custom samples\")\n",
    "\n",
    "print(\"üìö Using default book dataset from Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_processing"
   },
   "source": [
    "### üîß Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_preprocessor"
   },
   "outputs": [],
   "source": [
    "# Copy the data preprocessor code directly (for Colab compatibility)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import random\n",
    "\n",
    "class BookDataPreprocessor:\n",
    "    \"\"\"Preprocesses book data into SQuAD-like Q&A format for T5 training.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_context_length: int = 512, max_question_length: int = 128):\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_question_length = max_question_length\n",
    "        \n",
    "    def load_book_dataset(self) -> Dataset:\n",
    "        \"\"\"Load the Katharinelw/Book dataset.\"\"\"\n",
    "        try:\n",
    "            dataset = load_dataset(\"Katharinelw/Book\")\n",
    "            print(f\"Loaded dataset with {len(dataset['train'])} samples\")\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            return self._create_sample_dataset()\n",
    "    \n",
    "    def _create_sample_dataset(self) -> DatasetDict:\n",
    "        \"\"\"Create a sample dataset for testing purposes.\"\"\"\n",
    "        print(\"Creating sample dataset for testing\")\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"text\": \"The Great Gatsby is a novel by F. Scott Fitzgerald. It was published in 1925 and is set in the summer of 1922. The story follows Nick Carraway, who becomes neighbors with the mysterious Jay Gatsby. Gatsby is known for throwing lavish parties at his West Egg mansion.\",\n",
    "                \"title\": \"The Great Gatsby\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. The story takes place in the fictional town of Maycomb, Alabama, during the 1930s. It follows Scout Finch and her father Atticus, a lawyer who defends a Black man falsely accused of rape.\",\n",
    "                \"title\": \"To Kill a Mockingbird\"\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"1984 is a dystopian social science fiction novel by George Orwell. Published in 1949, it presents a totalitarian society ruled by Big Brother. The protagonist Winston Smith works for the Ministry of Truth, where he alters historical records.\",\n",
    "                \"title\": \"1984\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return DatasetDict({\n",
    "            \"train\": Dataset.from_list(sample_data * 15),  # More samples\n",
    "            \"validation\": Dataset.from_list(sample_data[:2])\n",
    "        })\n",
    "    \n",
    "    def generate_qa_pairs(self, text: str, title: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate Q&A pairs from book text using rule-based approach.\"\"\"\n",
    "        qa_pairs = []\n",
    "        \n",
    "        # Split text into sentences\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if len(sentence) < 20:\n",
    "                continue\n",
    "                \n",
    "            qa_pairs.extend(self._generate_questions_for_sentence(sentence, title, sentences))\n",
    "        \n",
    "        return qa_pairs\n",
    "    \n",
    "    def _generate_questions_for_sentence(self, sentence: str, title: str, context_sentences: List[str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate various types of questions for a given sentence.\"\"\"\n",
    "        qa_pairs = []\n",
    "        questions = []\n",
    "        \n",
    "        # Who questions - author\n",
    "        if 'by' in sentence:\n",
    "            author_match = re.search(r'by ([A-Z][a-z]+ [A-Z][a-z]+)', sentence)\n",
    "            if author_match:\n",
    "                author = author_match.group(1)\n",
    "                questions.append({\n",
    "                    \"question\": f\"Who wrote {title}?\",\n",
    "                    \"answer\": author\n",
    "                })\n",
    "        \n",
    "        # When questions - publication year\n",
    "        year_match = re.search(r'(\\d{4})', sentence)\n",
    "        if year_match:\n",
    "            year = year_match.group(1)\n",
    "            questions.append({\n",
    "                \"question\": f\"When was {title} published?\",\n",
    "                \"answer\": year\n",
    "            })\n",
    "        \n",
    "        # What questions - type\n",
    "        if 'novel' in sentence.lower():\n",
    "            genre_match = re.search(r'(\\w+\\s+)*novel', sentence.lower())\n",
    "            if genre_match:\n",
    "                genre = genre_match.group(0)\n",
    "                questions.append({\n",
    "                    \"question\": f\"What type of book is {title}?\",\n",
    "                    \"answer\": genre\n",
    "                })\n",
    "        \n",
    "        # Where questions - setting\n",
    "        location_patterns = [\n",
    "            r'in ([A-Z][a-z]+(?:, [A-Z][a-z]+)?)',\n",
    "            r'takes place in ([^,\\.]+)',\n",
    "            r'set in ([^,\\.]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in location_patterns:\n",
    "            location_match = re.search(pattern, sentence)\n",
    "            if location_match:\n",
    "                location = location_match.group(1).strip()\n",
    "                questions.append({\n",
    "                    \"question\": f\"Where is {title} set?\",\n",
    "                    \"answer\": location\n",
    "                })\n",
    "                break\n",
    "        \n",
    "        # Character questions\n",
    "        character_patterns = [\n",
    "            r'follows ([A-Z][a-z]+ [A-Z][a-z]+)',\n",
    "            r'protagonist ([A-Z][a-z]+ [A-Z][a-z]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in character_patterns:\n",
    "            char_match = re.search(pattern, sentence)\n",
    "            if char_match:\n",
    "                character = char_match.group(1)\n",
    "                questions.append({\n",
    "                    \"question\": f\"Who is the main character in {title}?\",\n",
    "                    \"answer\": character\n",
    "                })\n",
    "                break\n",
    "        \n",
    "        # Create context\n",
    "        context = ' '.join(context_sentences[:3])[:self.max_context_length]\n",
    "        \n",
    "        # Format for T5\n",
    "        for q in questions:\n",
    "            qa_pairs.append({\n",
    "                \"context\": context,\n",
    "                \"question\": q[\"question\"][:self.max_question_length],\n",
    "                \"answer\": q[\"answer\"],\n",
    "                \"title\": title\n",
    "            })\n",
    "        \n",
    "        return qa_pairs\n",
    "    \n",
    "    def format_for_t5(self, qa_pairs: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Format Q&A pairs for T5 text-to-text training.\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        for qa in qa_pairs:\n",
    "            input_text = f\"question: {qa['question']} context: {qa['context']}\"\n",
    "            target_text = qa['answer']\n",
    "            \n",
    "            formatted_data.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"target_text\": target_text,\n",
    "                \"question\": qa['question'],\n",
    "                \"context\": qa['context'],\n",
    "                \"answer\": qa['answer'],\n",
    "                \"title\": qa.get('title', '')\n",
    "            })\n",
    "        \n",
    "        return formatted_data\n",
    "    \n",
    "    def process_dataset(self, dataset: DatasetDict, num_samples: Optional[int] = None) -> DatasetDict:\n",
    "        \"\"\"Process the full dataset into T5-ready format.\"\"\"\n",
    "        processed_data = {\"train\": [], \"validation\": []}\n",
    "        \n",
    "        for split in [\"train\", \"validation\"]:\n",
    "            if split in dataset:\n",
    "                split_data = dataset[split]\n",
    "                if num_samples:\n",
    "                    split_data = split_data.select(range(min(num_samples, len(split_data))))\n",
    "                \n",
    "                for item in split_data:\n",
    "                    text = item.get('text', '')\n",
    "                    title = item.get('title', 'Unknown Book')\n",
    "                    \n",
    "                    qa_pairs = self.generate_qa_pairs(text, title)\n",
    "                    formatted_pairs = self.format_for_t5(qa_pairs)\n",
    "                    processed_data[split].extend(formatted_pairs)\n",
    "        \n",
    "        print(f\"Generated {len(processed_data['train'])} training samples\")\n",
    "        print(f\"Generated {len(processed_data['validation'])} validation samples\")\n",
    "        \n",
    "        return DatasetDict({\n",
    "            \"train\": Dataset.from_list(processed_data[\"train\"]),\n",
    "            \"validation\": Dataset.from_list(processed_data[\"validation\"])\n",
    "        })\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = BookDataPreprocessor()\n",
    "print(\"Data preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load and process the dataset\n",
    "print(\"üì• Loading book dataset...\")\n",
    "try:\n",
    "    raw_dataset = preprocessor.load_book_dataset()\n",
    "    print(\"‚úÖ Successfully loaded dataset from Hugging Face Hub\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading from Hub: {e}\")\n",
    "    print(\"üîÑ Falling back to sample dataset for demonstration\")\n",
    "    raw_dataset = preprocessor.load_book_dataset()  # Will use fallback\n",
    "\n",
    "print(\"\\nProcessing dataset into T5 format...\")\n",
    "processed_dataset = preprocessor.process_dataset(raw_dataset)\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "sample = processed_dataset[\"train\"][0]\n",
    "print(f\"Input: {sample['input_text']}\")\n",
    "print(f\"Target: {sample['target_text']}\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 3. Model Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5Config, \n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "class T5BookQAConfig:\n",
    "    \"\"\"Configuration for T5 book QA training.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model settings\n",
    "        self.model_name = \"t5-small\"\n",
    "        self.max_input_length = 512\n",
    "        self.max_output_length = 64\n",
    "        \n",
    "        # Training settings (optimized for Colab)\n",
    "        self.train_batch_size = 2 if torch.cuda.is_available() else 1  # Conservative for T4\n",
    "        self.eval_batch_size = 2 if torch.cuda.is_available() else 1\n",
    "        self.learning_rate = 3e-4\n",
    "        self.num_epochs = 2 if torch.cuda.is_available() else 1  # Faster training\n",
    "        self.warmup_steps = 100\n",
    "        self.weight_decay = 0.01\n",
"        \n",
"        # Memory optimization for Colab\n",
"        self.gradient_accumulation_steps = 2  # Simulate larger batch size\n",
"        self.dataloader_num_workers = 0  # Avoid multiprocessing issues in Colab\n",
"        self.max_grad_norm = 1.0  # Gradient clipping\n",
    "        \n",
    "        # Logging and saving\n",
    "        self.save_steps = 500\n",
    "        self.eval_steps = 500\n",
    "        self.logging_steps = 50\n",
    "\n",
    "config = T5BookQAConfig()\n",
    "print(f\"Using model: {config.model_name}\")\n",
    "print(f\"Training configuration: {config.num_epochs} epochs, batch size {config.train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load T5 model and tokenizer\n",
    "print(\"Loading T5 model and tokenizer...\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(config.model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenization"
   },
   "source": [
    "## 4. Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenize"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the dataset for T5 training.\"\"\"\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=config.max_input_length,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=config.max_output_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=processed_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Tokenized validation samples: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_args"
   },
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.train_batch_size,\n",
    "    per_device_eval_batch_size=config.eval_batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    weight_decay=config.weight_decay,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,  # Helps with memory on Colab\n",
"    dataloader_num_workers=config.dataloader_num_workers,  # Avoid multiprocessing issues\n",
"    gradient_accumulation_steps=config.gradient_accumulation_steps,  # Simulate larger batch\n",
"    max_grad_norm=config.max_grad_norm,  # Gradient clipping\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Enable mixed precision on GPU\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "subset_training"
   },
   "outputs": [],
   "source": [
    "# First, train on a small subset for sanity check\n",
    "print(\"\\nüöÄ Step 5: Training on subset first (sanity check)...\")\n",
    "\n",
    "# Create small subset for testing\n",
    "small_train = tokenized_dataset[\"train\"].select(range(20))\n",
    "small_val = tokenized_dataset[\"validation\"].select(range(5))\n",
    "\n",
    "# Quick training arguments for subset\n",
    "subset_args = TrainingArguments(\n",
    "    output_dir=\"./subset_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=5,\n",
    "    eval_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "# Create trainer for subset\n",
    "subset_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=subset_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train on subset\n",
    "print(\"Training on small subset...\")\n",
    "subset_trainer.train()\n",
    "\n",
    "print(\"‚úÖ Subset training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_subset"
   },
   "outputs": [],
   "source": [
    "# Test the model after subset training\n",
    "def generate_answer(question: str, context: str) -> str:\n",
    "    \"\"\"Generate answer using the trained model.\"\"\"\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=config.max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=config.max_output_length,\n",
    "            num_beams=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Test with sample questions\n",
    "test_cases = [\n",
    "    (\"Who wrote The Great Gatsby?\", \"The Great Gatsby is a novel by F. Scott Fitzgerald published in 1925.\"),\n",
    "    (\"When was 1984 published?\", \"1984 is a dystopian novel by George Orwell published in 1949.\"),\n",
    "    (\"What type of book is To Kill a Mockingbird?\", \"To Kill a Mockingbird is a novel by Harper Lee.\")\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing model after subset training:\")\n",
    "for i, (question, context) in enumerate(test_cases, 1):\n",
    "    answer = generate_answer(question, context)\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_training"
   },
   "outputs": [],
   "source": [
    "# Now proceed with full training\n",
    "print(\"\\nüöÄ Proceeding with full dataset training...\")\n",
    "\n",
    "# Create trainer for full dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting full training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Full training completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"\\nüìä Step 6: Evaluating trained model...\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Generate predictions for manual inspection\n",
    "print(\"\\nGenerating sample predictions...\")\n",
    "predictions = trainer.predict(tokenized_dataset[\"validation\"])\n",
    "\n",
    "# Decode predictions\n",
    "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
    "decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(min(5, len(decoded_preds))):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Predicted: {decoded_preds[i]}\")\n",
    "    print(f\"Reference: {decoded_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual_evaluation"
   },
   "outputs": [],
   "source": [
    "# Manual evaluation with more test cases\n",
    "print(\"\\nüß™ Manual evaluation with diverse test cases:\")\n",
    "\n",
    "extended_test_cases = [\n",
    "    (\"Who wrote The Great Gatsby?\", \"The Great Gatsby is a novel by F. Scott Fitzgerald published in 1925.\"),\n",
    "    (\"When was To Kill a Mockingbird published?\", \"To Kill a Mockingbird is a novel by Harper Lee published in 1960.\"),\n",
    "    (\"What type of book is 1984?\", \"1984 is a dystopian social science fiction novel by George Orwell.\"),\n",
    "    (\"Where is To Kill a Mockingbird set?\", \"The story takes place in the fictional town of Maycomb, Alabama, during the 1930s.\"),\n",
    "    (\"Who is the main character in The Great Gatsby?\", \"The story follows Nick Carraway, who becomes neighbors with Jay Gatsby.\"),\n",
    "]\n",
    "\n",
    "correct_answers = 0\n",
    "total_questions = len(extended_test_cases)\n",
    "\n",
    "for i, (question, context) in enumerate(extended_test_cases, 1):\n",
    "    answer = generate_answer(question, context)\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    \n",
    "    # Simple accuracy check (manual for now)\n",
    "    if any(word in answer.lower() for word in [\"fitzgerald\", \"1960\", \"dystopian\", \"alabama\", \"nick\"]):\n",
    "        correct_answers += 1\n",
    "        print(\"‚úÖ Reasonable answer\")\n",
    "    else:\n",
    "        print(\"‚ùå May need improvement\")\n",
    "\n",
    "print(f\"\\nApproximate accuracy: {correct_answers}/{total_questions} ({correct_answers/total_questions*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## 7. Save and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "# Use Google Drive in Colab for persistence\n",
    "try:\n",
    "    import google.colab\n",
    "    # Save to Google Drive in Colab\n",
    "    model_save_path = \"/content/drive/MyDrive/t5_book_qa_model\"\n",
    "    print(\"\\nüíæ Saving model to Google Drive for persistence...\")\n",
    "except ImportError:\n",
    "    # Save locally in non-Colab environments\n",
    "    model_save_path = \"./t5_book_qa_model\"\n",
    "    print(\"\\nüíæ Saving model locally...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "\n",
    "# Create a simple inference script for later use\n",
    "inference_script = '''import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def load_model(model_path):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def answer_question(model, tokenizer, device, question, context):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=64, num_beams=2)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage:\n",
    "# model, tokenizer, device = load_model(\"./t5_book_qa_model\")\n",
    "# answer = answer_question(model, tokenizer, device, \"Who wrote 1984?\", \"1984 is a novel by George Orwell.\")\n",
    "'''\n",
    "\n",
    "with open(f\"{model_save_path}/inference_example.py\", \"w\") as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"üìù Inference example script created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "integration"
   },
   "source": [
    "## 8. Hybrid System Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hybrid_routing"
   },
   "outputs": [],
   "source": [
    "# Example hybrid routing system\n",
    "class HybridQASystem:\n",
    "    \"\"\"Hybrid system that routes questions to SLM or LLM based on domain.\"\"\"\n",
    "    \n",
    "    def __init__(self, slm_model, slm_tokenizer, device):\n",
    "        self.slm_model = slm_model\n",
    "        self.slm_tokenizer = slm_tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # Keywords that indicate book-related questions\n",
    "        self.book_keywords = [\n",
    "            'book', 'novel', 'author', 'wrote', 'published', 'character', \n",
    "            'story', 'plot', 'chapter', 'setting', 'protagonist', 'literature'\n",
    "        ]\n",
    "    \n",
    "    def is_book_question(self, question: str) -> bool:\n",
    "        \"\"\"Determine if question is book-related.\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        return any(keyword in question_lower for keyword in self.book_keywords)\n",
    "    \n",
    "    def answer_with_slm(self, question: str, context: str) -> str:\n",
    "        \"\"\"Answer using the fine-tuned SLM.\"\"\"\n",
    "        return generate_answer(question, context)\n",
    "    \n",
    "    def answer_with_llm(self, question: str, context: str) -> str:\n",
    "        \"\"\"Placeholder for LLM answer (would integrate with API).\"\"\"\n",
    "        return f\"[LLM Response] This would be answered by a larger language model: {question}\"\n",
    "    \n",
    "    def route_and_answer(self, question: str, context: str = \"\") -> Dict[str, str]:\n",
    "        \"\"\"Route question to appropriate model and get answer.\"\"\"\n",
    "        if self.is_book_question(question):\n",
    "            answer = self.answer_with_slm(question, context)\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"model_used\": \"SLM (T5-small fine-tuned)\",\n",
    "                \"reasoning\": \"Question identified as book-related\"\n",
    "            }\n",
    "        else:\n",
    "            answer = self.answer_with_llm(question, context)\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"model_used\": \"LLM (General purpose)\",\n",
    "                \"reasoning\": \"Question identified as general knowledge\"\n",
    "            }\n",
    "\n",
    "# Initialize hybrid system\n",
    "hybrid_system = HybridQASystem(model, tokenizer, device)\n",
    "\n",
    "# Test routing\n",
    "test_questions = [\n",
    "    (\"Who wrote The Great Gatsby?\", \"The Great Gatsby is a novel by F. Scott Fitzgerald.\"),\n",
    "    (\"What is the capital of France?\", \"\"),\n",
    "    (\"What type of book is 1984?\", \"1984 is a dystopian novel by George Orwell.\"),\n",
    "    (\"How does photosynthesis work?\", \"\")\n",
    "]\n",
    "\n",
    "print(\"\\nüîÑ Step 7: Testing hybrid routing system:\")\n",
    "for question, context in test_questions:\n",
    "    result = hybrid_system.route_and_answer(question, context)\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Model used: {result['model_used']}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### ‚úÖ Completed Steps:\n",
    "1. **Task Definition**: Generative Q&A for book domain using T5-small\n",
    "2. **Data Preparation**: Converted book data to SQuAD-like Q&A pairs\n",
    "3. **Model Setup**: Configured T5-small with optimized parameters\n",
    "4. **Input Formatting**: Implemented text-to-text format for T5\n",
    "5. **Training**: Trained on subset first, then full dataset\n",
    "6. **Evaluation**: Assessed with manual testing and metrics\n",
    "7. **Integration**: Demonstrated hybrid SLM/LLM routing\n",
    "\n",
    "### üéØ Key Achievements:\n",
    "- Fine-tuned T5-small specifically for book question answering\n",
    "- Created SQuAD-compatible data processing pipeline\n",
    "- Implemented hybrid routing for domain vs. general questions\n",
    "- Optimized for Google Colab environment\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Scale up**: Train on larger book datasets\n",
    "2. **Improve accuracy**: Add more sophisticated evaluation metrics\n",
    "3. **Deploy**: Integrate with web application or API\n",
    "4. **Expand**: Add support for more literary domains\n",
    "5. **Optimize**: Further tune hyperparameters for better performance\n",
    "\n",
    "### üìÅ Generated Files:\n",
    "**Google Colab users**: Files are saved to Google Drive for persistence\n",
    "- `/content/drive/MyDrive/t5_book_qa_model/`: Fine-tuned model and tokenizer\n",
    "- `./results/`: Training checkpoints and logs (session-only)\n",
    "- `inference_example.py`: Example inference script\n",
    "\n",
    "**Local users**: Files are saved to current directory\n",
    "- `./t5_book_qa_model/`: Fine-tuned model and tokenizer\n",
    "- `./results/`: Training checkpoints and logs\n",
    "- `inference_example.py`: Example inference script\n",
    "\n",
    "### üéØ Google Colab Tips:\n",
    "1. **Model Persistence**: Your trained model is saved to Google Drive and will persist across sessions\n",
    "2. **Memory Management**: Use T4 GPU for optimal performance (15GB VRAM)\n",
    "3. **Session Timeout**: Colab sessions timeout after inactivity - models in Drive are safe\n",
    "4. **Restart Runtime**: If you encounter memory issues, restart runtime and re-run from the data loading section\n",
    "5. **Download Model**: Use `files.download('/content/drive/MyDrive/t5_book_qa_model.zip')` to download your trained model\n",
    "\n",
    "The model is now ready for production use in book-specific question answering tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_model"
   },
   "source": [
    "## 10. Download Trained Model (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Download the trained model for use outside Colab\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"üì¶ Preparing model for download...\")\n",
    "    \n",
    "    # Create a zip file of the model\n",
    "    model_zip_path = \"/content/t5_book_qa_model.zip\"\n",
    "    shutil.make_archive(\n",
    "        \"/content/t5_book_qa_model\", \n",
    "        'zip', \n",
    "        '/content/drive/MyDrive/t5_book_qa_model'\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model packaged as zip file\")\n",
    "    print(f\"üì• Downloading {model_zip_path}...\")\n",
    "    \n",
    "    # Download the zip file\n",
    "    files.download(model_zip_path)\n",
    "    \n",
    "    print(\"üéâ Model download complete!\")\n",
    "    print(\"\\nTo use the model locally:\")\n",
    "    print(\"1. Extract the zip file\")\n",
    "    print(\"2. Load with: T5ForConditionalGeneration.from_pretrained('path/to/extracted/model')\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Download feature is only available in Google Colab\")\n",
    "    print(f\"üìÅ Your model is saved locally at: {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}